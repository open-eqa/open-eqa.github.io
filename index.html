<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenEQA: Embodied Question Answering in the Era of Foundation Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">OpenEQA: Embodied Question Answering in the Era of
                            Foundation Models</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://arjunmajum.github.io/">Arjun Majumdar</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://anuragajay.github.io/">Anurag Ajay</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://keke-220.github.io/">Xiaohan Zhang</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/pranav-putta-3512b47a">Pranav Putta</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://yvsriram.github.io/">Sriram Yenamandra</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.mikaelhenaff.com/">Mikael Henaff</a>,
                            </span>
                            <span class="author-block">
                                <a href="http://ssilwal.com/">Sneha Silwal</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/paul-mcvay-277564204">Paul Mcvay</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.maksymets.com/">Oleksandr Maksymets</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/sergio-arnaud-226456198">Sergio Arnaud</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.karmeshyadav.com/">Karmesh Yadav</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://colinqiyangli.github.io/">Qiyang Li</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.andrew.cmu.edu/user/bnewman1/">Ben Newman</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://mohitsharma0690.github.io/">Mohit Sharma</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/vincentpierreberges">Vincent Berges</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.cs.binghamton.edu/~szhang/">Shiqi Zhang</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://yonatanbisk.com/">Yonatan Bisk</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/mrinalkalakrishnan/">Mrinal Kalakrishnan</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://fmeier.github.io/">Franziska Meier</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://cpaxton.github.io/about/">Chris Paxton</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://alexsax.github.io/">Sasha Sax</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Work done at Fundamental AI Research (FAIR), Meta </span>
                            <span class="author-block"><sup>*</sup>Equal Contributions</span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a target="_blank" href="https://open-eqa.github.io/assets/pdfs/paper.pdf"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                    <a target="_blank" href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-blog"></i>
                                        </span>
                                        <span>Blog</span>
                                    </a>
                                    <a target="_blank" href="https://github.com/facebookresearch/open-eqa"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                    <a target="_blank" href="https://github.com/facebookresearch/open-eqa"
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Benchmark</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="hero teaser">
        <div class="container is-max-widescreen">
            <div class="column content">
                <video poster="" autoplay muted loop height="100%" width="100%">
                    <source src="assets/videos/open-eqa-teaser.mp4" type="video/mp4">
                </video>
            </div>
            <h2 class="subtitle has-text-centered">
                Illustration of an episode history along with questions and answers from our <b>OpenEQA
                    benchmark</b>,
                which contains 1600+ untemplated questions that tests several aspects of open vocabulary embodied question answering.
            </h2>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 110%">
                            We present a modern formulation of Embodied Question Answering (EQA) as the task of understanding an environment well enough to answer questions about it in natural language. An agent can achieve such an understanding by either drawing upon episodic memory, exemplified by agents on smart glasses, or by actively exploring the environment, as in the case of mobile robots. We accompany our formulation with OpenEQA - the first open-vocabulary benchmark dataset for EQA supporting both episodic memory and active exploration use cases. OpenEQA contains over 1600 high-quality human generated questions drawn from over 180 real-world environments. In addition to the dataset, we also provide an automatic LLM-powered evaluation protocol that has excellent correlation with human judgement. Using this dataset and evaluation protocol, we evaluate several state-of-the-art foundation models like GPT-4V and find that they significantly lag behind human-level performance.  Consequently, OpenEQA stands out as a straightforward, measurable, and practically relevant benchmark that poses a considerable challenge to current generation of AI models. We hope this inspires and stimulates future research at the intersection of Embodied AI, conversational agents, and world models.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvc1">OpenEQA Dataset Statistics</span></h2>
                        <img src="assets/images/figure-3.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="100%">
                        <span style="font-size: 110%">
                            <span style="font-weight: bold">
                                <br />
                                Example questions and dataset statistics of OpenEQA.</span> The episode history <i>H</i>
                            provides a human-like tour of a home. EQA agents must answer diverse, human-generated
                            questions <i>Q</i> from 7 EQA categories, aiming match the ground answers <i>A*</i>. Tours
                            are collected from diverse environments including home and office locations (not shown
                            above).
                            Dataset statistics (right) break down the question distribution by video source (top),
                            question category (middle), and
                            episodic memory vs active setting. Note that, by design, the HM3D questions are shared
                            across the EM-EQA and A-EQA settings.
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="columns is-centered">

                <div class="column">
                    <h2 class="title is-3">Evaluation Workflow</h2>
                    <div class="columns is-centered">
                        <div class="column content">

                            <img src="assets/images/workflow.png" class="interpolation-image" alt=""
                                style="display: block; margin-left: auto; margin-right: auto" width="100%">
                            <span style="font-size: 110%">
                                <span style="font-weight: bold">
                                    <br />
                                    <br />
                                    Illustration of LLM-Match evaluation and workflow. </span>
                                While the open-vocabulary nature makes EQA realistic, it poses a challenge for
                                evaluation
                                due to multiplicity of correct answers. One approach to evaluation is human trials, but
                                it
                                can be prohibitively slow and expensive, especially for benchmarks. As an alternative,
                                we
                                use an LLM to evaluate the correctness of open-vocabulary answers produced by EQA
                                agents.
                            </span>
                        </div>

                    </div>
                </div>
                <div class="column">
                    <h2 class="title is-3">Baseline Performance</h2>
                    <div class="columns is-centered">
                        <div class="column content">

                            <img src="assets/images/category-level_performance.png" class="interpolation-image" alt=""
                                style="display: block; margin-left: auto; margin-right: auto" width="80%">
                            <span style="font-size: 110%">
                                <span style="font-weight: bold">
                                    <br />
                                    Category-level performance on EM-EQA. </span>
                                We find that agents with access to visual information excel at localizing and
                                recognizing objects and attributes, and make better use of this information to answer
                                questions that require world knowledge. However, on other categories performance is
                                closer to the blind LLM baseline (GPT-4), indicating
                                substantial room for improvement on OpenEQA.
                            </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
    @inproceedings{OpenEQA2023,
        title         = {OpenEQA: Embodied Question Answering in the Era of Foundation Models}, 
        booktitle     = {Conference on Computer Vision and Pattern Recognition (CVPR)},
        author        = {Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and Yadav, Karmesh and Li, Qiyang and Newman, Ben and Sharma, Mohit and Berges, Vincent and Zhang, Shiqi and Agrawal, Pulkit and Bisk, Yonatan and Batra, Dhruv and Kalakrishnan, Mrinal and Meier, Franziska and Paxton, Chris and Sax, Sasha and Rajeswaran, Aravind},
        year          = {2024},
    }
        </code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                                href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>