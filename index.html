<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenEQA: Embodied Question Answering in the Era of Foundation Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./assets/images/favicon.svg">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title">OpenEQA: Embodied Question Answering in the Era of
                            Foundation Models</h1>
                        <div class="is-size-5 publication-authors">
                            <span class="author-block">
                                <a href="https://arjunmajum.github.io/">Arjun Majumdar</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://anuragajay.github.io/">Anurag Ajay</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://keke-220.github.io/">Xiaohan Zhang</a><sup>*</sup>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/pranav-putta-3512b47a">Pranav Putta</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://yvsriram.github.io/">Sriram Yenamandra</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.mikaelhenaff.com/">Mikael Henaff</a>,
                            </span>
                            <span class="author-block">
                                <a href="http://ssilwal.com/">Sneha Silwal</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/paul-mcvay-277564204">Paul Mcvay</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.maksymets.com/">Oleksandr Maksymets</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/sergio-arnaud-226456198">Sergio Arnaud</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.karmeshyadav.com/">Karmesh Yadav</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://colinqiyangli.github.io/">Qiyang Li</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.andrew.cmu.edu/user/bnewman1/">Ben Newman</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://mohitsharma0690.github.io/">Mohit Sharma</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/vincentpierreberges">Vincent Berges</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.cs.binghamton.edu/~szhang/">Shiqi Zhang</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://yonatanbisk.com/">Yonatan Bisk</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://faculty.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://www.linkedin.com/in/mrinalkalakrishnan/">Mrinal Kalakrishnan</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://fmeier.github.io/">Franziska Meier</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://cpaxton.github.io/about/">Chris Paxton</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://alexsax.github.io/">Sasha Sax</a>,
                            </span>
                            <span class="author-block">
                                <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a>
                            </span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Work done at Fundamental AI Research (FAIR), Meta, </span>
                            <span class="author-block"><sup>*</sup>Equal Contribution</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            (Code, benchmark, and dataset coming soon)
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- TODO PDF Link. -->
                                <span class="link-block">
                                    <a target="_blank" href=""
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>


                                <!-- Code Link. -->
                                <span class="link-block">
                                    <a target="_blank" href=""
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                    <a target="_blank" href=""
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-robot"></i>
                                        </span>
                                        <span>Benchmark</span>
                                    </a>
                                    <a target="_blank" href=""
                                        class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fas fa-database"></i>
                                        </span>
                                        <span>Dataset</span>
                                    </a>
                                </span>
                            </div>

                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <img src="assets/images/fig1_nolines.png" class="interpolation-image" alt=""
                        style="display: block; margin-left: auto; margin-right: auto" width="100%">
                    <span style="font-size: 100%">
                        <br />
                        Illustration of an episode history along with questions and answers from our <b>OpenEQA
                            benchmark</b>,
                        which contains 1600+ untemplated questions that test aspects of attribute recognition, spatial
                        understanding, functional reasoning, and world knowledge. In episodic-memory EQA (EM-EQA),
                        agents parse a sequence of historical sensory observations, and in active EQA (A-EQA), agents
                        must explore real-world scanned environments to gather information to answer questions. Natural
                        language answers are scored using our proposed LLM-Match metric, which showed excellent
                        agreement with human scoring in a double-blind study.
                </div>
            </div>
        </div>
    </section> -->

    <section class="hero teaser">
        <div class="container is-max-desktop">
            <div class="hero-body">
                <img src="assets/images/fig1_nolines.png" class="interpolation-image" alt=""
                    style="display: block; margin-left: auto; margin-right: auto" width="100%">
                </br>
                <h2 class="subtitle has-text-centered">
                    Illustration of an episode history along with questions and answers from our <b>OpenEQA
                        benchmark</b>,
                    which contains 1600+ untemplated questions that test aspects of attribute recognition, spatial
                    understanding, functional reasoning, and world knowledge.
                </h2>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p style="font-size: 125%">
                            Embodied Question Answering (EQA) requires an agent to answer questions by drawing upon
                            either its episodic memory (e.g. agent on smart glasses) or by actively exploring the
                            environment to collect necessary information (e.g. mobile robots).
                            We present a modern formulation of EQA and a corresponding benchmark that are relevant in
                            the era of LLMs; both questions and answers are open-vocabulary (e.g. "Q: Where did I leave
                            my keys? A: On the coffee table").
                            Our evaluation dataset contains over 1600 high-quality human generated questions
                            representative of real-world use cases for Embodied AI agents. In addition to the dataset,
                            we also provide an automatic evaluation protocol powered by LLMs. Using this dataset and
                            evaluation protocol, we study the performance of different EQA agents powered by state of
                            the art foundation models. Overall, we find that even the best performing models (e.g.
                            GPT-4V) lag behind human-level performance on our benchmark, underscoring its utility and
                            relevance to the research community.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvc1">Overview</span></h2>
                        <img src="assets/images/figure-3.png" class="interpolation-image" alt=""
                            style="display: block; margin-left: auto; margin-right: auto" width="100%">
                        <span style="font-size: 110%">
                            <span style="font-weight: bold">
                                <br />
                                Example questions and dataset statistics of OpenEQA.</span> The episode history <i>H</i>
                            provides a human-like tour of a home. EQA agents must answer diverse, human-generated
                            questions $Q$ from 7 EQA categories, aiming match the ground answers <i>A*</i>. Tours are
                            collected from diverse environments including home and office locations (not shown above).
                            Additional dataset examples are in~\cref{app:dataset-examples}. Dataset statistics (right)
                            break down the question distribution by video source (top), question category (middle), and
                            episodic memory vs active setting. Note that, by design, the HM3D questions are shared
                            across the EM-EQA and A-EQA settings.
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!--Conclusion-->
    <section class="section">
        <div class="container is-max-widescreen">
            <div class="rows">
                <div class="rows is-centered ">
                    <div class="row is-full-width">
                        <h2 class="title is-3"><span class="dvc1">Conclusion</span></h2>
                        <div class="content has-text-justified">

                            <p style="font-size: 125%">
                                We introduce OpenEQA, the first realistic benchmark to study open-vocabulary EQA in both
                                episodic memory and active settings. Specifically, OpenEQA includes challenging,
                                human-generated, open-ended questions that require understanding an environment and
                                answering question in natural language. Our benchmark is primarily enabled by (1) videos
                                and scans of real-world indoor environments and (2) LLMs that can be used for scoring
                                open-ended answers in an efficient and reliable manner, as we demonstrated through our
                                analyses. We use OpenEQA to benchmark foundation models such as GPT-4V and combinations
                                of foundation models that include image captioning or scene-graph construction methods.
                                Ultimately, we find a large gap between the best models and human-level performance.
                                These results demonstrate that OpenEQA is well positioned (in terms of difficulty) to
                                serve as barometer for tracking future progress in EQA and multimodal learning.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
    </section>


    <section class="section" id="BibTeX">
        <div class="container is-max-widescreen content">
            <h2 class="title">BibTeX</h2>
            <pre><code>
    @inproceedings{OpenEQA2023,
        title         = {OpenEQA: Embodied Question Answering in the Era of Foundation Models}, 
        author        = {},
        year          = {2023},
        eprint        = {},
        archivePrefix = {arXiv},
        primaryClass  = {cs.CV}
    }
        </code></pre>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <div class="columns is-centered">
                <div class="column">
                    <div class="content has-text-centered">
                        <p>
                            Website template borrowed from <a
                                href="https://github.com/nerfies/nerfies.github.io">NeRFies</a> and <a
                                href="https://github.com/cliport/cliport.github.io">CLIPort</a>.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </footer>

</body>

</html>